{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNqHAEHAkxHyd/XZNWKZSnP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6f0fc282fcc7492f9f69b7006c965da1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c33c768a2e2d44ccaff15770bc816ffd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ed3426a74d5b41529ed2415d51db74ec",
              "IPY_MODEL_17682c2c3d2c4e58b725e67519742072"
            ]
          }
        },
        "c33c768a2e2d44ccaff15770bc816ffd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ed3426a74d5b41529ed2415d51db74ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9dc111e1d45f4e3c893d8a70b71085a5",
            "_dom_classes": [],
            "description": "Words Decoded: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4502,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4502,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a6ea52cc8574ae1a9a06e4597632b34"
          }
        },
        "17682c2c3d2c4e58b725e67519742072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_501390cca6ff49ae8433669160ca8557",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4502/4502 [1:44:14&lt;00:00,  1.39s/Words]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9d32c6229341458aa734726b0eeef60d"
          }
        },
        "9dc111e1d45f4e3c893d8a70b71085a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a6ea52cc8574ae1a9a06e4597632b34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "501390cca6ff49ae8433669160ca8557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9d32c6229341458aa734726b0eeef60d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da683327fe1d42a2a907302bcf2e6394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_010bc0799d614441b874a14e345d9b83",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8c630759cdcd432fb3cecf2c4d868277",
              "IPY_MODEL_4e561f61cd114375a2e4c16639bd6e31"
            ]
          }
        },
        "010bc0799d614441b874a14e345d9b83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8c630759cdcd432fb3cecf2c4d868277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6af4babe8d6d409aa59e73b575242a85",
            "_dom_classes": [],
            "description": "Words Decoded: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4502,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4502,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_66f94f9bcf134d93bdc7da566352a8ba"
          }
        },
        "4e561f61cd114375a2e4c16639bd6e31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0bb2fd9c738147faaf14a33dd66804a7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4502/4502 [2:01:04&lt;00:00,  1.61s/Words]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b6ec847a5ee9466d8570407ab7b7e44d"
          }
        },
        "6af4babe8d6d409aa59e73b575242a85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "66f94f9bcf134d93bdc7da566352a8ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0bb2fd9c738147faaf14a33dd66804a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b6ec847a5ee9466d8570407ab7b7e44d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6962e09ae59d452a869d83ab8a82becd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2e59bc5b897f47fa90718f50fc7670ab",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8ef120bf435d41adada129422e713a69",
              "IPY_MODEL_f5af25ddaf9e479295ee5b2b9b4fc984"
            ]
          }
        },
        "2e59bc5b897f47fa90718f50fc7670ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ef120bf435d41adada129422e713a69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ba6acd0c16d44c5284d28cd34a5139c8",
            "_dom_classes": [],
            "description": "Words Decoded: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b5bb52fbaccd42e6a81db3cc6f17c357"
          }
        },
        "f5af25ddaf9e479295ee5b2b9b4fc984": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_87a49e4697ad4d628bc837339de14d36",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:02&lt;00:00,  2.27s/Words]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8d34a96bd1a1437b87c4ea91580c88dc"
          }
        },
        "ba6acd0c16d44c5284d28cd34a5139c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b5bb52fbaccd42e6a81db3cc6f17c357": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "87a49e4697ad4d628bc837339de14d36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8d34a96bd1a1437b87c4ea91580c88dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYckdLcqn7fj"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqVl1jqjhFS1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eecc8b0-1462-4afa-c5ee-b109f40d3ab3"
      },
      "source": [
        "!pip install tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.font_manager import FontProperties\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Input\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense, AdditiveAttention, Concatenate, TimeDistributed, Layer, RepeatVector\n",
        "from keras.models import Model"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZhWli67hM3t"
      },
      "source": [
        "# Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTBE2iGhhJhZ"
      },
      "source": [
        "def prepare_data(data_path, batch_size, num_samples):\n",
        "  with tf.device('/device:GPU:0'):  \n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "    input_characters = set()\n",
        "    target_characters = set()\n",
        "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.read().split(\"\\n\")\n",
        "        print()\n",
        "    for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "        target_text, input_text, _ = line.split(\"\\t\")\n",
        "        # We use \"tab\" as the \"start sequence\" character\n",
        "        # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "        target_text = \"\\t\" + target_text.strip(\"\\t\\n \") + \"\\n\"\n",
        "        input_texts.append(input_text.strip(\"\\t\\n \"))\n",
        "        target_texts.append(target_text)\n",
        "        for char in input_text:\n",
        "            if char not in input_characters:\n",
        "                input_characters.add(char)\n",
        "        for char in target_text:\n",
        "            if char not in target_characters:\n",
        "                target_characters.add(char)\n",
        "\n",
        "    input_characters = sorted(list(input_characters)+[' '])\n",
        "    target_characters = sorted(list(target_characters)+[' '])\n",
        "    num_encoder_tokens = len(input_characters)\n",
        "    num_decoder_tokens = len(target_characters)\n",
        "    max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "    max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "    # print(\"Number of samples:\", len(input_texts))\n",
        "    # print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "    # print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "    # print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "    # print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "    input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "    target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_encoder_seq_length), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_target_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "        for t, char in enumerate(input_text):\n",
        "            encoder_input_data[i, t] = input_token_index[char]\n",
        "        encoder_input_data[i, t+1:] = input_token_index[' ']\n",
        "\n",
        "        for t, char in enumerate(target_text):\n",
        "            decoder_input_data[i, t] = target_token_index[char]\n",
        "        decoder_input_data[i, t+1:] = input_token_index[' ']\n",
        "\n",
        "        for t, char in enumerate(target_text):        \n",
        "            if t > 0:\n",
        "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "        decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "    return input_texts, target_texts, encoder_input_data, decoder_input_data, decoder_target_data, num_encoder_tokens,num_decoder_tokens,\\\n",
        "          max_encoder_seq_length, max_decoder_seq_length, input_token_index, target_token_index\n",
        "              "
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLron0P5oIBV"
      },
      "source": [
        "# Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdAuCKzbhYty"
      },
      "source": [
        "## Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy6TQuPUhQPV"
      },
      "source": [
        "def build_model(encoder_input_size, decoder_input_size, embedding_size, latent_dim, hidden_layers, input_vocab, target_vocab, dropout, rec_dropout, cell_type='LSTM'):\n",
        "  with tf.device('/device:GPU:0'):  \n",
        "    # Encoder Inputs\n",
        "    encoder_inputs = Input(shape=(encoder_input_size))\n",
        "    encoder_embeddings = Embedding(input_vocab, embedding_size)(encoder_inputs)\n",
        "    #Decoder Inputs\n",
        "    decoder_inputs = Input(shape=(decoder_input_size))\n",
        "    decoder_embeddings = Embedding(target_vocab, embedding_size)(decoder_inputs)\n",
        "\n",
        "    if cell_type == 'RNN':\n",
        "      # Encoder    \n",
        "      encoder_RNNs = [SimpleRNN(latent_dim, dropout= dropout, recurrent_dropout=rec_dropout, return_state=True, return_sequences=True) for _ in range(hidden_layers)]\n",
        "      encoder_states = []\n",
        "      encoder_output, state_h = encoder_RNNs[0](encoder_embeddings)\n",
        "      encoder_states.extend([state_h])\n",
        "      for i in range(1, hidden_layers):\n",
        "        encoder_output, state_h_e = encoder_RNNs[i](encoder_output)\n",
        "        encoder_states.append(state_h_e)\n",
        "\n",
        "      # Decoder    \n",
        "      decoder_RNNs, decoder_states = [], []\n",
        "      decoder_RNNs.append(SimpleRNN(latent_dim, dropout= dropout, recurrent_dropout=rec_dropout, return_state=True, return_sequences=True))\n",
        "      decoder_output, state_h = decoder_RNNs[0](decoder_embeddings, initial_state=encoder_states[0])\n",
        "      decoder_states.append(state_h)\n",
        "      for i in range(1, hidden_layers):\n",
        "        decoder_RNNs.append(SimpleRNN(latent_dim, dropout= dropout, recurrent_dropout=rec_dropout, return_sequences=True, return_state=True))\n",
        "        decoder_output, state_h_d = decoder_RNNs[i](decoder_output, initial_state=encoder_states[i])\n",
        "        decoder_states.append(state_h_d)\n",
        "\n",
        "      decoder_dense = keras.layers.Dense(target_vocab, activation=\"softmax\")\n",
        "      decoder_output = decoder_dense(decoder_output)\n",
        "      model = Model([encoder_inputs, decoder_inputs], decoder_output)\n",
        "\n",
        "      # Encoder (Inference)\n",
        "      encoder_states_i = []\n",
        "      encoder_output_i, state_h_i = encoder_RNNs[0](encoder_embeddings)\n",
        "      encoder_states_i.append(state_h_i)\n",
        "      for i in range(1, hidden_layers):\n",
        "        encoder_output_i, state_h_e, state_c_e = encoder_RNNs[i](encoder_output_i)\n",
        "        encoder_states_i.append(state_h_e)\n",
        "      encoder_model = Model(encoder_inputs, encoder_states_i)\n",
        "\n",
        "      # Decoder (Inference)\n",
        "      decoder_states_inputs = [keras.Input(shape=(latent_dim)) for _ in range(hidden_layers)]\n",
        "      decoder_states_i = []\n",
        "      decoder_output_i, state_h_d_i = decoder_RNNs[0](decoder_embeddings, initial_state=decoder_states_inputs[0])\n",
        "      decoder_states_i.append(state_h_d_i)\n",
        "      for i in range(1, hidden_layers):\n",
        "        decoder_output_i, state_h_d_i = decoder_RNNs[i](decoder_output_i, initial_state=decoder_states_inputs[i])\n",
        "        decoder_states_i.append(state_h_d_i)\n",
        "\n",
        "      decoder_output_i = decoder_dense(decoder_output_i)\n",
        "      decoder_model = keras.models.Model([decoder_inputs] + decoder_states_inputs, [decoder_output_i] + decoder_states_i)\n",
        "\n",
        "    elif cell_type == 'GRU':\n",
        "      # Encoder    \n",
        "      encoder_GRUs = [GRU(latent_dim, dropout=dropout, recurrent_dropout=rec_dropout, return_state=True, return_sequences=True) for _ in range(hidden_layers)]\n",
        "      encoder_states = []\n",
        "      encoder_output, state_h = encoder_GRUs[0](encoder_embeddings)\n",
        "      encoder_states.extend([state_h])\n",
        "      for i in range(1, hidden_layers):\n",
        "        encoder_output, state_h_e = encoder_GRUs[i](encoder_output)\n",
        "        encoder_states.append(state_h_e)\n",
        "\n",
        "      # Decoder    \n",
        "      decoder_GRUs, decoder_states = [], []\n",
        "      decoder_GRUs.append(GRU(latent_dim, dropout=dropout, recurrent_dropout=rec_dropout, return_state=True, return_sequences=True))\n",
        "      decoder_output, state_h = decoder_GRUs[0](decoder_embeddings, initial_state=encoder_states[0])\n",
        "      decoder_states.append(state_h)\n",
        "      for i in range(1, hidden_layers):\n",
        "        decoder_GRUs.append(GRU(latent_dim, dropout=dropout, recurrent_dropout=rec_dropout, return_sequences=True, return_state=True))\n",
        "        decoder_output, state_h_d = decoder_GRUs[i](decoder_output, initial_state=encoder_states[i])\n",
        "        decoder_states.append(state_h_d)\n",
        "\n",
        "      decoder_dense = keras.layers.Dense(target_vocab, activation=\"softmax\")\n",
        "      decoder_output = decoder_dense(decoder_output)\n",
        "      model = Model([encoder_inputs, decoder_inputs], decoder_output)\n",
        "\n",
        "      # Encoder (Inference)\n",
        "      encoder_states_i = []\n",
        "      encoder_output_i, state_h_i = encoder_GRUs[0](encoder_embeddings)\n",
        "      encoder_states_i.append(state_h_i)\n",
        "      for i in range(1, hidden_layers):\n",
        "        encoder_output_i, state_h_e = encoder_GRUs[i](encoder_output_i)\n",
        "        encoder_states_i.append(state_h_e)\n",
        "      encoder_model = Model(encoder_inputs, encoder_states_i)\n",
        "\n",
        "      # Decoder (Inference)\n",
        "      decoder_states_inputs = [keras.Input(shape=(latent_dim)) for _ in range(hidden_layers)]\n",
        "      decoder_states_i = []\n",
        "      decoder_output_i, state_h_d_i = decoder_GRUs[0](decoder_embeddings, initial_state=decoder_states_inputs[0])\n",
        "      decoder_states_i.append(state_h_d_i)\n",
        "      for i in range(1, hidden_layers):\n",
        "        decoder_output_i, state_h_d_i = decoder_GRUs[i](decoder_output_i, initial_state=decoder_states_inputs[i])\n",
        "        decoder_states_i.append(state_h_d_i)\n",
        "\n",
        "      decoder_output_i = decoder_dense(decoder_output_i)\n",
        "      decoder_model = keras.models.Model([decoder_inputs] + decoder_states_inputs, [decoder_output_i] + decoder_states_i)\n",
        "\n",
        "    else:\n",
        "      # Encoder\n",
        "      encoder_inputs = Input(shape=(encoder_input_size))\n",
        "      encoder_embeddings = Embedding(input_vocab, embedding_size)(encoder_inputs)\n",
        "      encoder_LSTMs = [LSTM(latent_dim, dropout=dropout, recurrent_dropout=rec_dropout, return_state=True, return_sequences=True) for _ in range(hidden_layers)]\n",
        "      encoder_states = []\n",
        "      encoder_output, state_h, state_c = encoder_LSTMs[0](encoder_embeddings)\n",
        "      encoder_states.extend([state_h, state_c])\n",
        "      for i in range(1, hidden_layers):\n",
        "        encoder_output, state_h_e, state_c_e = encoder_LSTMs[i](encoder_output)\n",
        "        encoder_states.extend([state_h_e, state_c_e])\n",
        "\n",
        "      # Decoder\n",
        "      decoder_LSTMs, decoder_states = [], []\n",
        "      decoder_LSTMs.append(LSTM(latent_dim, dropout=dropout, recurrent_dropout=rec_dropout, return_state=True, return_sequences=True))\n",
        "      decoder_output, state_h, state_c = decoder_LSTMs[0](decoder_embeddings, initial_state=encoder_states[:2])\n",
        "      decoder_states.extend([state_h, state_c])\n",
        "      for i in range(1, hidden_layers):\n",
        "        decoder_LSTMs.append(LSTM(latent_dim, dropout=dropout, recurrent_dropout=rec_dropout, return_sequences=True, return_state=True))\n",
        "        decoder_output, state_h_d, state_c_d = decoder_LSTMs[i](decoder_output, initial_state=encoder_states[2*i:][:2])\n",
        "        decoder_states.extend([state_h_d, state_c_d])\n",
        "\n",
        "      decoder_dense = keras.layers.Dense(target_vocab, activation=\"softmax\")\n",
        "      decoder_output = decoder_dense(decoder_output)\n",
        "      model = Model([encoder_inputs, decoder_inputs], decoder_output)\n",
        "\n",
        "      # Encoder (Inference)\n",
        "      encoder_states_i = []\n",
        "      encoder_output_i, state_h_i, state_c_i = encoder_LSTMs[0](encoder_embeddings)\n",
        "      encoder_states_i.extend([state_h_i, state_c_i])\n",
        "      for i in range(1, hidden_layers):\n",
        "        encoder_output_i, state_h_e, state_c_e = encoder_LSTMs[i](encoder_output_i)\n",
        "        encoder_states_i.extend([state_h_e, state_c_e])\n",
        "      encoder_model = Model(encoder_inputs, encoder_states_i)\n",
        "\n",
        "      # Decoder (Inference)\n",
        "      decoder_states_inputs = [keras.Input(shape=(latent_dim)) for _ in range(2*hidden_layers)]\n",
        "      decoder_states_i = []\n",
        "      decoder_output_i, state_h_d_i, state_c_d_i = decoder_LSTMs[0](decoder_embeddings, initial_state=decoder_states_inputs[:2])\n",
        "      decoder_states_i.extend([state_h_d_i, state_c_d_i])\n",
        "      for i in range(1, hidden_layers):\n",
        "        decoder_output_i, state_h_d_i, state_c_d_i = decoder_LSTMs[i](decoder_output_i, initial_state=decoder_states_inputs[2*i:][:2])\n",
        "        decoder_states_i.extend([state_h_d_i, state_c_d_i])\n",
        "\n",
        "      decoder_output_i = decoder_dense(decoder_output_i)\n",
        "      decoder_model = keras.models.Model([decoder_inputs] + decoder_states_inputs, [decoder_output_i] + decoder_states_i)\n",
        "\n",
        "    return model, encoder_model, decoder_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O38atUDc_MZy"
      },
      "source": [
        "input_texts, target_text, encoder_input_data, decoder_input_data, decoder_target_data, num_encoder_tokens,num_decoder_tokens,\\\n",
        " max_encoder_seq_length, max_decoder_seq_length, input_token_index, target_token_index = prepare_data(data_path, batch_size, num_samples)\n",
        "\n",
        "model, encoder_model, decoder_model= build_model(max_encoder_seq_length, max_decoder_seq_length, embedding_size,\n",
        "                                                 latent_dim, num_encoder_tokens, num_decoder_tokens, cell_type= 'LSTM') \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EARX7DJb1uXj"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnQj8MmC_88e"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVWEl0kzr1yt"
      },
      "source": [
        "## Using the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6yFvPEcr-cN"
      },
      "source": [
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLiQye-nr_qe"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index[\"\\t\"]\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        decoder_output = decoder_model.predict([target_seq] + [states_value])\n",
        "        # print(decoder_output)\n",
        "        output_tokens, states = decoder_output[0], decoder_output[1:]\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        # target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [states]\n",
        "    return decoded_sentence    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iZyM3YIsCxs"
      },
      "source": [
        "for seq_index in range(10):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(\"-\")\n",
        "    print(\"Input sentence:\", input_texts[seq_index])\n",
        "    print(\"Decoded sentence:\", decoded_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgNctRqGo8JO"
      },
      "source": [
        "# Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRcSLytVmxKQ"
      },
      "source": [
        "## Wandb Integration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDpc1NNe5j8U"
      },
      "source": [
        "!pip install --upgrade wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOuXw7Ttmz3-"
      },
      "source": [
        "!wandb login dd888f73500a67fc53f9191092b22f3946ac0e02"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqY9-cBIm60b"
      },
      "source": [
        "# Init wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "wandb.init(project=\"assignment-3\", entity=\"ravi-kumar\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w28eqSQUnHvr"
      },
      "source": [
        "## Wandb Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUhPtvc6nPJH"
      },
      "source": [
        "# Configure the sweep – specify the parameters to search through, the search strategy, the optimization metric et all.\n",
        "sweep_config = {\n",
        "    'method': 'random', #grid, random\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'cell_type' : {\n",
        "            'values': ['RNN', 'GRU', 'LSTM']\n",
        "        },\n",
        "        'embedding_size':{\n",
        "            'values': [32, 64, 128, 256]\n",
        "        },\n",
        "        'hidden_layers' :{\n",
        "            'values': [1, 2, 3]\n",
        "        },\n",
        "        'hidden_layer_size' :{\n",
        "            'values': [32, 64, 128, 256]\n",
        "        },\n",
        "        'dropout' :{\n",
        "            'values': [0.2, 0.3]\n",
        "        },\n",
        "        'batch_size' : {\n",
        "            'values': [64, 128, 256]\n",
        "        },\n",
        "        'epochs':{\n",
        "            'values': [10, 20]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5MH_vVhnYzJ"
      },
      "source": [
        "## Running Wandb Sweeps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5sk3zaEua58"
      },
      "source": [
        "def train():\n",
        "  with tf.device('/device:GPU:0'):    \n",
        "    # Wandb Configuration\n",
        "    config_defaults = {\n",
        "        'cell_type' : 'RNN',\n",
        "        'embedding_size': 128,\n",
        "        'hidden_layers' : 1,\n",
        "        'hidden_layer_size' : 128,\n",
        "        'dropout' : 0.3,\n",
        "        'rec_dropout' : 0.2,\n",
        "        'batch_size' : 64,\n",
        "        'epochs': 10\n",
        "    }\n",
        "    wandb.init(config=config_defaults)\n",
        "    config= wandb.config\n",
        "\n",
        "    # Parameters\n",
        "    batch_size = config.batch_size\n",
        "    epochs = config.epochs\n",
        "    latent_dim = config.hidden_layer_size\n",
        "    embedding_size = config.embedding_size\n",
        "    hidden_layers = config.hidden_layers\n",
        "    cell_type = config.cell_type\n",
        "    dropout = config.dropout\n",
        "    rec_dropout = 0 #config.rec_dropout\n",
        "    num_samples = 44205\n",
        "    # Path to the data txt file on disk.\n",
        "    data_path = \"/content/drive/MyDrive/Deep Learning/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "\n",
        "    # Preparing/Loading Data\n",
        "    input_texts, target_text, encoder_input_data, decoder_input_data, decoder_target_data, num_encoder_tokens,num_decoder_tokens,\\\n",
        "    max_encoder_seq_length, max_decoder_seq_length, input_token_index, target_token_index = prepare_data(data_path, batch_size, num_samples)\n",
        "\n",
        "    # Building Model\n",
        "    model, encoder_model, decoder_model= build_model(max_encoder_seq_length, max_decoder_seq_length, embedding_size,\n",
        "                                                 latent_dim, hidden_layers, num_encoder_tokens, num_decoder_tokens, dropout, rec_dropout, cell_type= cell_type) \n",
        "    \n",
        "    # Training Model\n",
        "    model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    model.fit(\n",
        "        [encoder_input_data, decoder_input_data],\n",
        "        decoder_target_data,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[WandbCallback()]\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8ijxmqMnWjn",
        "outputId": "5344c303-fb3c-4cb5-eb27-57356158f2a9"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=\"ravi-kumar\", project=\"assignment-3\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: d71k4yrm\n",
            "Sweep URL: https://wandb.ai/ravi-kumar/assignment-3/sweeps/d71k4yrm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wIekE-Yz-A_"
      },
      "source": [
        "wandb.agent(sweep_id, train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP2ZNYICpUp3"
      },
      "source": [
        "# Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3OfY8hz2sRs"
      },
      "source": [
        "## Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff6C23TS2vMO"
      },
      "source": [
        "def build_model(encoder_input_size, decoder_input_size, embedding_size, latent_dim, hidden_layers, input_vocab, target_vocab, dropout, rec_dropout, cell_type='LSTM'):\n",
        "  with tf.device('/device:GPU:0'):  \n",
        "    # Encoder Inputs\n",
        "    encoder_inputs = Input(shape=(encoder_input_size))\n",
        "    encoder_embeddings = Embedding(input_vocab, embedding_size)(encoder_inputs)\n",
        "    #Decoder Inputs\n",
        "    decoder_inputs = Input(shape=(decoder_input_size))\n",
        "    decoder_embeddings = Embedding(target_vocab, embedding_size)(decoder_inputs)\n",
        "\n",
        "    if cell_type == 'RNN':\n",
        "      # Encoder    \n",
        "      encoder_RNNs = [SimpleRNN(latent_dim, dropout= dropout, recurrent_dropout=rec_dropout, return_state=True, return_sequences=True) for _ in range(hidden_layers)]\n",
        "      encoder_states = []\n",
        "      encoder_output, state_h = encoder_RNNs[0](encoder_embeddings)\n",
        "      encoder_states.extend([state_h])\n",
        "      for i in range(1, hidden_layers):\n",
        "        encoder_output, state_h_e = encoder_RNNs[i](encoder_output)\n",
        "        encoder_states.append(state_h_e)\n",
        "\n",
        "      # Decoder    \n",
        "      decoder_RNNs, decoder_states = [], []\n",
        "      decoder_RNNs.append(SimpleRNN(latent_dim, dropout= dropout, recurrent_dropout=rec_dropout, return_state=True, return_sequences=True))\n",
        "      decoder_output, state_h = decoder_RNNs[0](decoder_embeddings, initial_state=encoder_states[0])\n",
        "      decoder_states.append(state_h)\n",
        "      for i in range(1, hidden_layers):\n",
        "        decoder_RNNs.append(SimpleRNN(latent_dim, dropout= dropout, recurrent_dropout=rec_dropout, return_sequences=True, return_state=True))\n",
        "        decoder_output, state_h_d = decoder_RNNs[i](decoder_output, initial_state=encoder_states[i])\n",
        "        decoder_states.append(state_h_d)\n",
        "\n",
        "      decoder_dense = keras.layers.Dense(target_vocab, activation=\"softmax\")\n",
        "      decoder_output = decoder_dense(decoder_output)\n",
        "      model = Model([encoder_inputs, decoder_inputs], decoder_output)\n",
        "\n",
        "      # Encoder (Inference)\n",
        "      encoder_states_i = []\n",
        "      encoder_output_i, state_h_i = encoder_RNNs[0](encoder_embeddings)\n",
        "      encoder_states_i.append(state_h_i)\n",
        "      for i in range(1, hidden_layers):\n",
        "        encoder_output_i, state_h_e, state_c_e = encoder_RNNs[i](encoder_output_i)\n",
        "        encoder_states_i.append(state_h_e)\n",
        "      encoder_model = Model(encoder_inputs, encoder_states_i)\n",
        "\n",
        "      # Decoder (Inference)\n",
        "      decoder_states_inputs = [keras.Input(shape=(latent_dim)) for _ in range(hidden_layers)]\n",
        "      decoder_states_i = []\n",
        "      decoder_output_i, state_h_d_i = decoder_RNNs[0](decoder_embeddings, initial_state=decoder_states_inputs[0])\n",
        "      decoder_states_i.append(state_h_d_i)\n",
        "      for i in range(1, hidden_layers):\n",
        "        decoder_output_i, state_h_d_i = decoder_RNNs[i](decoder_output_i, initial_state=decoder_states_inputs[i])\n",
        "        decoder_states_i.append(state_h_d_i)\n",
        "\n",
        "      decoder_output_i = decoder_dense(decoder_output_i)\n",
        "      decoder_model = keras.models.Model([decoder_inputs] + decoder_states_inputs, [decoder_output_i] + decoder_states_i)\n",
        "\n",
        "    elif cell_type == 'GRU':\n",
        "      # Encoder    \n",
        "      encoder_GRUs = [GRU(latent_dim, dropout=dropout, recurrent_dropout=rec_dropout, return_state=True, return_sequences=True) for _ in range(hidden_layers)]\n",
        "      encoder_states = []\n",
        "      encoder_output, state_h = encoder_GRUs[0](encoder_embeddings)\n",
        "      encoder_states.extend([state_h])\n",
        "      for i in range(1, hidden_layers):\n",
        "        encoder_output, state_h_e = encoder_GRUs[i](encoder_output)\n",
        "        encoder_states.append(state_h_e)\n",
        "\n",
        "      # Decoder    \n",
        "      decoder_GRUs, decoder_states = [], []\n",
        "      decoder_GRUs.append(GRU(latent_dim, dropout=dropout, recurrent_dropout=rec_dropout, return_state=True, return_sequences=True))\n",
        "      decoder_output, state_h = decoder_GRUs[0](decoder_embeddings, initial_state=encoder_states[0])\n",
        "      decoder_states.append(state_h)\n",
        "      for i in range(1, hidden_layers):\n",
        "        decoder_GRUs.append(GRU(latent_dim, dropout=dropout, recurrent_dropout=rec_dropout, return_sequences=True, return_state=True))\n",
        "        decoder_output, state_h_d = decoder_GRUs[i](decoder_output, initial_state=encoder_states[i])\n",
        "        decoder_states.append(state_h_d)\n",
        "\n",
        "      decoder_dense = keras.layers.Dense(target_vocab, activation=\"softmax\")\n",
        "      decoder_output = decoder_dense(decoder_output)\n",
        "      model = Model([encoder_inputs, decoder_inputs], decoder_output)\n",
        "\n",
        "      # Encoder (Inference)\n",
        "      encoder_states_i = []\n",
        "      encoder_output_i, state_h_i = encoder_GRUs[0](encoder_embeddings)\n",
        "      encoder_states_i.append(state_h_i)\n",
        "      for i in range(1, hidden_layers):\n",
        "        encoder_output_i, state_h_e = encoder_GRUs[i](encoder_output_i)\n",
        "        encoder_states_i.append(state_h_e)\n",
        "      encoder_model = Model(encoder_inputs, encoder_states_i)\n",
        "\n",
        "      # Decoder (Inference)\n",
        "      decoder_states_inputs = [keras.Input(shape=(latent_dim)) for _ in range(hidden_layers)]\n",
        "      decoder_states_i = []\n",
        "      decoder_output_i, state_h_d_i = decoder_GRUs[0](decoder_embeddings, initial_state=decoder_states_inputs[0])\n",
        "      decoder_states_i.append(state_h_d_i)\n",
        "      for i in range(1, hidden_layers):\n",
        "        decoder_output_i, state_h_d_i = decoder_GRUs[i](decoder_output_i, initial_state=decoder_states_inputs[i])\n",
        "        decoder_states_i.append(state_h_d_i)\n",
        "\n",
        "      decoder_output_i = decoder_dense(decoder_output_i)\n",
        "      decoder_model = keras.models.Model([decoder_inputs] + decoder_states_inputs, [decoder_output_i] + decoder_states_i)\n",
        "\n",
        "    else:\n",
        "      # Encoder\n",
        "      encoder_inputs = Input(shape=(encoder_input_size))\n",
        "      encoder_embeddings = Embedding(input_vocab, embedding_size)(encoder_inputs)\n",
        "      encoder_LSTMs = [LSTM(latent_dim, dropout=dropout, recurrent_dropout=rec_dropout, return_state=True, return_sequences=True) for _ in range(hidden_layers)]\n",
        "      encoder_states = []\n",
        "      encoder_output, state_h, state_c = encoder_LSTMs[0](encoder_embeddings)\n",
        "      encoder_states.extend([state_h, state_c])\n",
        "      for i in range(1, hidden_layers):\n",
        "        encoder_output, state_h_e, state_c_e = encoder_LSTMs[i](encoder_output)\n",
        "        encoder_states.extend([state_h_e, state_c_e])\n",
        "\n",
        "      # Decoder\n",
        "      decoder_LSTMs, decoder_states = [], []\n",
        "      decoder_LSTMs.append(LSTM(latent_dim, dropout=dropout, recurrent_dropout=rec_dropout, return_state=True, return_sequences=True))\n",
        "      decoder_output, state_h, state_c = decoder_LSTMs[0](decoder_embeddings, initial_state=encoder_states[:2])\n",
        "      decoder_states.extend([state_h, state_c])\n",
        "      for i in range(1, hidden_layers):\n",
        "        decoder_LSTMs.append(LSTM(latent_dim, dropout=dropout, recurrent_dropout=rec_dropout, return_sequences=True, return_state=True))\n",
        "        decoder_output, state_h_d, state_c_d = decoder_LSTMs[i](decoder_output, initial_state=encoder_states[2*i:][:2])\n",
        "        decoder_states.extend([state_h_d, state_c_d])\n",
        "\n",
        "      decoder_dense = keras.layers.Dense(target_vocab, activation=\"softmax\")\n",
        "      decoder_output = decoder_dense(decoder_output)\n",
        "      model = Model([encoder_inputs, decoder_inputs], decoder_output)\n",
        "\n",
        "      # Encoder (Inference)\n",
        "      encoder_states_i = []\n",
        "      encoder_output_i, state_h_i, state_c_i = encoder_LSTMs[0](encoder_embeddings)\n",
        "      encoder_states_i.extend([state_h_i, state_c_i])\n",
        "      for i in range(1, hidden_layers):\n",
        "        encoder_output_i, state_h_e, state_c_e = encoder_LSTMs[i](encoder_output_i)\n",
        "        encoder_states_i.extend([state_h_e, state_c_e])\n",
        "      encoder_model = Model(encoder_inputs, encoder_states_i)\n",
        "\n",
        "      # Decoder (Inference)\n",
        "      decoder_states_inputs = [keras.Input(shape=(latent_dim)) for _ in range(2*hidden_layers)]\n",
        "      decoder_states_i = []\n",
        "      decoder_output_i, state_h_d_i, state_c_d_i = decoder_LSTMs[0](decoder_embeddings, initial_state=decoder_states_inputs[:2])\n",
        "      decoder_states_i.extend([state_h_d_i, state_c_d_i])\n",
        "      for i in range(1, hidden_layers):\n",
        "        decoder_output_i, state_h_d_i, state_c_d_i = decoder_LSTMs[i](decoder_output_i, initial_state=decoder_states_inputs[2*i:][:2])\n",
        "        decoder_states_i.extend([state_h_d_i, state_c_d_i])\n",
        "\n",
        "      decoder_output_i = decoder_dense(decoder_output_i)\n",
        "      decoder_model = keras.models.Model([decoder_inputs] + decoder_states_inputs, [decoder_output_i] + decoder_states_i)\n",
        "\n",
        "    return model, encoder_model, decoder_model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4AfjNHupt6A"
      },
      "source": [
        "## Testing the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTyyPKFT6X_D"
      },
      "source": [
        "### Preparing Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex3V1UyP6I1p"
      },
      "source": [
        "def prepare_test_data(data_path, batch_size, num_samples, input_token_index, target_token_index):\n",
        "  with tf.device('/device:GPU:0'):  \n",
        "    input_texts, target_texts, targets = [], [], []\n",
        "    \n",
        "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.read().split(\"\\n\")\n",
        "    for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "        target_text, input_text, _ = line.split(\"\\t\")\n",
        "        # We use \"tab\" as the \"start sequence\" character\n",
        "        # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "        targets.append(target_text.strip(\"\\t\\n \"))\n",
        "        target_text = \"\\t\" + target_text.strip(\"\\t\\n \") + \"\\n\"\n",
        "        input_texts.append(input_text.strip(\"\\t\\n \"))\n",
        "        target_texts.append(target_text)\n",
        "\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_encoder_seq_length), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_target_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "        for t, char in enumerate(input_text):\n",
        "            encoder_input_data[i, t] = input_token_index[char]\n",
        "        encoder_input_data[i, t+1:] = input_token_index[' ']          \n",
        "\n",
        "        for t, char in enumerate(target_text):\n",
        "            decoder_input_data[i, t] = target_token_index[char]\n",
        "        decoder_input_data[i, t+1:] = target_token_index[' ']          \n",
        "\n",
        "        for t, char in enumerate(target_text):        \n",
        "            if t > 0:\n",
        "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "        decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "    return input_texts, targets, encoder_input_data, decoder_input_data, decoder_target_data\n",
        "              "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z-4I5iFuxRF"
      },
      "source": [
        "### Training the Model with best Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwcGpKy5mipH",
        "outputId": "1ec4b8b5-9ab7-4678-bdec-18fb64dc440f"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  # Parameters\n",
        "  batch_size = 64\n",
        "  epochs = 150\n",
        "  latent_dim = 64  # hidden layer size\n",
        "  embedding_size = 256\n",
        "  hidden_layers = 2\n",
        "  cell_type = 'LSTM'\n",
        "  dropout = 0.3\n",
        "  rec_dropout = 0\n",
        "  num_samples = 44205\n",
        "\n",
        "  # Path to the data txt file on disk.\n",
        "  train_data_path = \"/content/drive/MyDrive/Deep Learning/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "  test_data_path = \"/content/drive/MyDrive/Deep Learning/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\"\n",
        "\n",
        "  # Preparing/Loading Data\n",
        "  input_texts, target_text, encoder_input_data, decoder_input_data, decoder_target_data, num_encoder_tokens, num_decoder_tokens,\\\n",
        "        max_encoder_seq_length, max_decoder_seq_length, input_token_index, target_token_index = prepare_data(train_data_path, batch_size, num_samples)\n",
        "\n",
        "  # Building Model\n",
        "  model, encoder_model, decoder_model = build_model(max_encoder_seq_length, max_decoder_seq_length, embedding_size,\n",
        "                                                      latent_dim, hidden_layers, num_encoder_tokens, num_decoder_tokens, dropout, rec_dropout, cell_type=cell_type)\n",
        "\n",
        "  # Training Model\n",
        "  model.compile(\n",
        "      optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "  )\n",
        "  model.fit(\n",
        "      [encoder_input_data, decoder_input_data],\n",
        "      decoder_target_data,\n",
        "      batch_size=batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_split=0.2,\n",
        "      # callbacks=[EarlyStopping(monitor='val_loss')]\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/150\n",
            "553/553 [==============================] - 14s 16ms/step - loss: 1.2822 - accuracy: 0.7025 - val_loss: 1.0930 - val_accuracy: 0.7254\n",
            "Epoch 2/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.8603 - accuracy: 0.7656 - val_loss: 1.0381 - val_accuracy: 0.7502\n",
            "Epoch 3/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.7140 - accuracy: 0.8005 - val_loss: 0.9094 - val_accuracy: 0.7712\n",
            "Epoch 4/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.5846 - accuracy: 0.8311 - val_loss: 0.8130 - val_accuracy: 0.7895\n",
            "Epoch 5/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.4928 - accuracy: 0.8533 - val_loss: 0.7109 - val_accuracy: 0.8066\n",
            "Epoch 6/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.4171 - accuracy: 0.8733 - val_loss: 0.6726 - val_accuracy: 0.8183\n",
            "Epoch 7/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.3621 - accuracy: 0.8886 - val_loss: 0.5898 - val_accuracy: 0.8375\n",
            "Epoch 8/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.3235 - accuracy: 0.8993 - val_loss: 0.5723 - val_accuracy: 0.8452\n",
            "Epoch 9/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.2961 - accuracy: 0.9074 - val_loss: 0.5433 - val_accuracy: 0.8513\n",
            "Epoch 10/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.2752 - accuracy: 0.9133 - val_loss: 0.5250 - val_accuracy: 0.8478\n",
            "Epoch 11/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.2578 - accuracy: 0.9184 - val_loss: 0.5076 - val_accuracy: 0.8576\n",
            "Epoch 12/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.2450 - accuracy: 0.9221 - val_loss: 0.5222 - val_accuracy: 0.8570\n",
            "Epoch 13/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.2331 - accuracy: 0.9261 - val_loss: 0.4856 - val_accuracy: 0.8647\n",
            "Epoch 14/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.2201 - accuracy: 0.9297 - val_loss: 0.5141 - val_accuracy: 0.8646\n",
            "Epoch 15/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.2145 - accuracy: 0.9317 - val_loss: 0.4973 - val_accuracy: 0.8691\n",
            "Epoch 16/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.2063 - accuracy: 0.9343 - val_loss: 0.5050 - val_accuracy: 0.8695\n",
            "Epoch 17/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1994 - accuracy: 0.9361 - val_loss: 0.5095 - val_accuracy: 0.8644\n",
            "Epoch 18/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1943 - accuracy: 0.9381 - val_loss: 0.5104 - val_accuracy: 0.8703\n",
            "Epoch 19/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1879 - accuracy: 0.9395 - val_loss: 0.5156 - val_accuracy: 0.8677\n",
            "Epoch 20/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1832 - accuracy: 0.9409 - val_loss: 0.4936 - val_accuracy: 0.8725\n",
            "Epoch 21/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1786 - accuracy: 0.9426 - val_loss: 0.5100 - val_accuracy: 0.8724\n",
            "Epoch 22/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1741 - accuracy: 0.9442 - val_loss: 0.5245 - val_accuracy: 0.8711\n",
            "Epoch 23/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1715 - accuracy: 0.9446 - val_loss: 0.5260 - val_accuracy: 0.8674\n",
            "Epoch 24/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1687 - accuracy: 0.9457 - val_loss: 0.5230 - val_accuracy: 0.8696\n",
            "Epoch 25/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1642 - accuracy: 0.9469 - val_loss: 0.5096 - val_accuracy: 0.8706\n",
            "Epoch 26/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1618 - accuracy: 0.9480 - val_loss: 0.5349 - val_accuracy: 0.8734\n",
            "Epoch 27/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1603 - accuracy: 0.9482 - val_loss: 0.5360 - val_accuracy: 0.8678\n",
            "Epoch 28/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1566 - accuracy: 0.9491 - val_loss: 0.5433 - val_accuracy: 0.8677\n",
            "Epoch 29/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1558 - accuracy: 0.9498 - val_loss: 0.5342 - val_accuracy: 0.8731\n",
            "Epoch 30/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1522 - accuracy: 0.9510 - val_loss: 0.5477 - val_accuracy: 0.8698\n",
            "Epoch 31/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1491 - accuracy: 0.9515 - val_loss: 0.5281 - val_accuracy: 0.8728\n",
            "Epoch 32/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1473 - accuracy: 0.9522 - val_loss: 0.5348 - val_accuracy: 0.8718\n",
            "Epoch 33/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1454 - accuracy: 0.9530 - val_loss: 0.5515 - val_accuracy: 0.8683\n",
            "Epoch 34/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1438 - accuracy: 0.9535 - val_loss: 0.5525 - val_accuracy: 0.8690\n",
            "Epoch 35/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1424 - accuracy: 0.9538 - val_loss: 0.5641 - val_accuracy: 0.8655\n",
            "Epoch 36/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1394 - accuracy: 0.9549 - val_loss: 0.5579 - val_accuracy: 0.8699\n",
            "Epoch 37/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1387 - accuracy: 0.9551 - val_loss: 0.5490 - val_accuracy: 0.8654\n",
            "Epoch 38/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1374 - accuracy: 0.9555 - val_loss: 0.5333 - val_accuracy: 0.8725\n",
            "Epoch 39/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1367 - accuracy: 0.9556 - val_loss: 0.5560 - val_accuracy: 0.8723\n",
            "Epoch 40/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1344 - accuracy: 0.9566 - val_loss: 0.5410 - val_accuracy: 0.8690\n",
            "Epoch 41/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1330 - accuracy: 0.9570 - val_loss: 0.5777 - val_accuracy: 0.8633\n",
            "Epoch 42/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1325 - accuracy: 0.9569 - val_loss: 0.5544 - val_accuracy: 0.8703\n",
            "Epoch 43/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1309 - accuracy: 0.9577 - val_loss: 0.5718 - val_accuracy: 0.8657\n",
            "Epoch 44/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1297 - accuracy: 0.9578 - val_loss: 0.5843 - val_accuracy: 0.8659\n",
            "Epoch 45/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1279 - accuracy: 0.9583 - val_loss: 0.5796 - val_accuracy: 0.8672\n",
            "Epoch 46/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.1275 - accuracy: 0.9585 - val_loss: 0.5753 - val_accuracy: 0.8683\n",
            "Epoch 47/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1258 - accuracy: 0.9588 - val_loss: 0.5835 - val_accuracy: 0.8672\n",
            "Epoch 48/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1252 - accuracy: 0.9591 - val_loss: 0.6050 - val_accuracy: 0.8674\n",
            "Epoch 49/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1256 - accuracy: 0.9589 - val_loss: 0.6054 - val_accuracy: 0.8646\n",
            "Epoch 50/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1233 - accuracy: 0.9598 - val_loss: 0.5923 - val_accuracy: 0.8668\n",
            "Epoch 51/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1231 - accuracy: 0.9599 - val_loss: 0.5917 - val_accuracy: 0.8658\n",
            "Epoch 52/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1206 - accuracy: 0.9607 - val_loss: 0.6081 - val_accuracy: 0.8667\n",
            "Epoch 53/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1208 - accuracy: 0.9605 - val_loss: 0.6325 - val_accuracy: 0.8677\n",
            "Epoch 54/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1188 - accuracy: 0.9614 - val_loss: 0.6114 - val_accuracy: 0.8646\n",
            "Epoch 55/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1195 - accuracy: 0.9612 - val_loss: 0.6243 - val_accuracy: 0.8616\n",
            "Epoch 56/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1182 - accuracy: 0.9614 - val_loss: 0.6130 - val_accuracy: 0.8649\n",
            "Epoch 57/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1180 - accuracy: 0.9616 - val_loss: 0.6121 - val_accuracy: 0.8664\n",
            "Epoch 58/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1163 - accuracy: 0.9620 - val_loss: 0.6155 - val_accuracy: 0.8680\n",
            "Epoch 59/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1165 - accuracy: 0.9620 - val_loss: 0.6437 - val_accuracy: 0.8648\n",
            "Epoch 60/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1146 - accuracy: 0.9623 - val_loss: 0.6372 - val_accuracy: 0.8638\n",
            "Epoch 61/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1141 - accuracy: 0.9627 - val_loss: 0.6560 - val_accuracy: 0.8605\n",
            "Epoch 62/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1140 - accuracy: 0.9627 - val_loss: 0.6415 - val_accuracy: 0.8650\n",
            "Epoch 63/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1122 - accuracy: 0.9631 - val_loss: 0.6365 - val_accuracy: 0.8662\n",
            "Epoch 64/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1114 - accuracy: 0.9635 - val_loss: 0.6366 - val_accuracy: 0.8674\n",
            "Epoch 65/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1123 - accuracy: 0.9633 - val_loss: 0.6370 - val_accuracy: 0.8628\n",
            "Epoch 66/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1119 - accuracy: 0.9633 - val_loss: 0.6632 - val_accuracy: 0.8615\n",
            "Epoch 67/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1109 - accuracy: 0.9638 - val_loss: 0.6556 - val_accuracy: 0.8683\n",
            "Epoch 68/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1098 - accuracy: 0.9639 - val_loss: 0.6694 - val_accuracy: 0.8667\n",
            "Epoch 69/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1087 - accuracy: 0.9639 - val_loss: 0.6675 - val_accuracy: 0.8643\n",
            "Epoch 70/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1080 - accuracy: 0.9642 - val_loss: 0.6815 - val_accuracy: 0.8643\n",
            "Epoch 71/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.1075 - accuracy: 0.9650 - val_loss: 0.6645 - val_accuracy: 0.8610\n",
            "Epoch 72/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.1084 - accuracy: 0.9645 - val_loss: 0.6655 - val_accuracy: 0.8654\n",
            "Epoch 73/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1072 - accuracy: 0.9648 - val_loss: 0.6819 - val_accuracy: 0.8606\n",
            "Epoch 74/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.1055 - accuracy: 0.9652 - val_loss: 0.6670 - val_accuracy: 0.8630\n",
            "Epoch 75/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.1060 - accuracy: 0.9651 - val_loss: 0.6885 - val_accuracy: 0.8612\n",
            "Epoch 76/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.1060 - accuracy: 0.9651 - val_loss: 0.6804 - val_accuracy: 0.8625\n",
            "Epoch 77/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1039 - accuracy: 0.9656 - val_loss: 0.6784 - val_accuracy: 0.8639\n",
            "Epoch 78/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1046 - accuracy: 0.9657 - val_loss: 0.6674 - val_accuracy: 0.8643\n",
            "Epoch 79/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1037 - accuracy: 0.9658 - val_loss: 0.6888 - val_accuracy: 0.8626\n",
            "Epoch 80/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1036 - accuracy: 0.9660 - val_loss: 0.6776 - val_accuracy: 0.8667\n",
            "Epoch 81/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.1027 - accuracy: 0.9664 - val_loss: 0.6760 - val_accuracy: 0.8663\n",
            "Epoch 82/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.1031 - accuracy: 0.9659 - val_loss: 0.6759 - val_accuracy: 0.8644\n",
            "Epoch 83/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.1018 - accuracy: 0.9663 - val_loss: 0.6803 - val_accuracy: 0.8654\n",
            "Epoch 84/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.1011 - accuracy: 0.9663 - val_loss: 0.6798 - val_accuracy: 0.8658\n",
            "Epoch 85/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.1007 - accuracy: 0.9669 - val_loss: 0.6832 - val_accuracy: 0.8654\n",
            "Epoch 86/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.1005 - accuracy: 0.9668 - val_loss: 0.6925 - val_accuracy: 0.8646\n",
            "Epoch 87/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.1010 - accuracy: 0.9666 - val_loss: 0.6820 - val_accuracy: 0.8685\n",
            "Epoch 88/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.1004 - accuracy: 0.9668 - val_loss: 0.6997 - val_accuracy: 0.8645\n",
            "Epoch 89/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0992 - accuracy: 0.9671 - val_loss: 0.6789 - val_accuracy: 0.8658\n",
            "Epoch 90/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0989 - accuracy: 0.9672 - val_loss: 0.6993 - val_accuracy: 0.8646\n",
            "Epoch 91/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.0983 - accuracy: 0.9674 - val_loss: 0.7120 - val_accuracy: 0.8606\n",
            "Epoch 92/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.0990 - accuracy: 0.9672 - val_loss: 0.6986 - val_accuracy: 0.8624\n",
            "Epoch 93/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0987 - accuracy: 0.9674 - val_loss: 0.7053 - val_accuracy: 0.8609\n",
            "Epoch 94/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0972 - accuracy: 0.9676 - val_loss: 0.7179 - val_accuracy: 0.8637\n",
            "Epoch 95/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.0974 - accuracy: 0.9678 - val_loss: 0.7015 - val_accuracy: 0.8628\n",
            "Epoch 96/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.0974 - accuracy: 0.9675 - val_loss: 0.7230 - val_accuracy: 0.8620\n",
            "Epoch 97/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0966 - accuracy: 0.9678 - val_loss: 0.7178 - val_accuracy: 0.8616\n",
            "Epoch 98/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0968 - accuracy: 0.9678 - val_loss: 0.7040 - val_accuracy: 0.8644\n",
            "Epoch 99/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0967 - accuracy: 0.9677 - val_loss: 0.7094 - val_accuracy: 0.8617\n",
            "Epoch 100/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0966 - accuracy: 0.9681 - val_loss: 0.7373 - val_accuracy: 0.8585\n",
            "Epoch 101/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0958 - accuracy: 0.9683 - val_loss: 0.7258 - val_accuracy: 0.8638\n",
            "Epoch 102/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0943 - accuracy: 0.9685 - val_loss: 0.7466 - val_accuracy: 0.8603\n",
            "Epoch 103/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0956 - accuracy: 0.9683 - val_loss: 0.7241 - val_accuracy: 0.8603\n",
            "Epoch 104/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0940 - accuracy: 0.9688 - val_loss: 0.7402 - val_accuracy: 0.8572\n",
            "Epoch 105/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.0957 - accuracy: 0.9681 - val_loss: 0.7143 - val_accuracy: 0.8612\n",
            "Epoch 106/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0942 - accuracy: 0.9688 - val_loss: 0.6955 - val_accuracy: 0.8655\n",
            "Epoch 107/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0944 - accuracy: 0.9688 - val_loss: 0.7157 - val_accuracy: 0.8650\n",
            "Epoch 108/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0943 - accuracy: 0.9685 - val_loss: 0.7458 - val_accuracy: 0.8586\n",
            "Epoch 109/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0920 - accuracy: 0.9696 - val_loss: 0.7385 - val_accuracy: 0.8639\n",
            "Epoch 110/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0926 - accuracy: 0.9693 - val_loss: 0.7081 - val_accuracy: 0.8643\n",
            "Epoch 111/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0923 - accuracy: 0.9690 - val_loss: 0.7160 - val_accuracy: 0.8598\n",
            "Epoch 112/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0909 - accuracy: 0.9695 - val_loss: 0.7374 - val_accuracy: 0.8586\n",
            "Epoch 113/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0913 - accuracy: 0.9697 - val_loss: 0.7609 - val_accuracy: 0.8567\n",
            "Epoch 114/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0919 - accuracy: 0.9692 - val_loss: 0.7530 - val_accuracy: 0.8582\n",
            "Epoch 115/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0914 - accuracy: 0.9695 - val_loss: 0.7757 - val_accuracy: 0.8554\n",
            "Epoch 116/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0905 - accuracy: 0.9696 - val_loss: 0.7426 - val_accuracy: 0.8604\n",
            "Epoch 117/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0915 - accuracy: 0.9695 - val_loss: 0.7465 - val_accuracy: 0.8627\n",
            "Epoch 118/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0912 - accuracy: 0.9695 - val_loss: 0.7456 - val_accuracy: 0.8627\n",
            "Epoch 119/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0906 - accuracy: 0.9697 - val_loss: 0.7549 - val_accuracy: 0.8612\n",
            "Epoch 120/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0895 - accuracy: 0.9703 - val_loss: 0.7687 - val_accuracy: 0.8623\n",
            "Epoch 121/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.0909 - accuracy: 0.9695 - val_loss: 0.7783 - val_accuracy: 0.8623\n",
            "Epoch 122/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0899 - accuracy: 0.9699 - val_loss: 0.7946 - val_accuracy: 0.8566\n",
            "Epoch 123/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0892 - accuracy: 0.9701 - val_loss: 0.7769 - val_accuracy: 0.8600\n",
            "Epoch 124/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.0895 - accuracy: 0.9701 - val_loss: 0.7582 - val_accuracy: 0.8639\n",
            "Epoch 125/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.0897 - accuracy: 0.9696 - val_loss: 0.7405 - val_accuracy: 0.8638\n",
            "Epoch 126/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0892 - accuracy: 0.9704 - val_loss: 0.7630 - val_accuracy: 0.8608\n",
            "Epoch 127/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.0890 - accuracy: 0.9706 - val_loss: 0.7527 - val_accuracy: 0.8656\n",
            "Epoch 128/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.0890 - accuracy: 0.9703 - val_loss: 0.7839 - val_accuracy: 0.8635\n",
            "Epoch 129/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.0878 - accuracy: 0.9706 - val_loss: 0.7846 - val_accuracy: 0.8585\n",
            "Epoch 130/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0859 - accuracy: 0.9710 - val_loss: 0.7793 - val_accuracy: 0.8615\n",
            "Epoch 131/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0887 - accuracy: 0.9706 - val_loss: 0.7807 - val_accuracy: 0.8610\n",
            "Epoch 132/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.0876 - accuracy: 0.9704 - val_loss: 0.7672 - val_accuracy: 0.8602\n",
            "Epoch 133/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0885 - accuracy: 0.9709 - val_loss: 0.7716 - val_accuracy: 0.8614\n",
            "Epoch 134/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0856 - accuracy: 0.9711 - val_loss: 0.7727 - val_accuracy: 0.8598\n",
            "Epoch 135/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.0876 - accuracy: 0.9707 - val_loss: 0.7592 - val_accuracy: 0.8642\n",
            "Epoch 136/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0873 - accuracy: 0.9708 - val_loss: 0.7795 - val_accuracy: 0.8611\n",
            "Epoch 137/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0867 - accuracy: 0.9708 - val_loss: 0.7943 - val_accuracy: 0.8597\n",
            "Epoch 138/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0868 - accuracy: 0.9710 - val_loss: 0.7780 - val_accuracy: 0.8641\n",
            "Epoch 139/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.0861 - accuracy: 0.9713 - val_loss: 0.8045 - val_accuracy: 0.8574\n",
            "Epoch 140/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0859 - accuracy: 0.9711 - val_loss: 0.8175 - val_accuracy: 0.8589\n",
            "Epoch 141/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0852 - accuracy: 0.9713 - val_loss: 0.8075 - val_accuracy: 0.8583\n",
            "Epoch 142/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0853 - accuracy: 0.9712 - val_loss: 0.8111 - val_accuracy: 0.8630\n",
            "Epoch 143/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0849 - accuracy: 0.9715 - val_loss: 0.8153 - val_accuracy: 0.8601\n",
            "Epoch 144/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.0859 - accuracy: 0.9711 - val_loss: 0.7905 - val_accuracy: 0.8627\n",
            "Epoch 145/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.0851 - accuracy: 0.9714 - val_loss: 0.8159 - val_accuracy: 0.8568\n",
            "Epoch 146/150\n",
            "553/553 [==============================] - 8s 14ms/step - loss: 0.0845 - accuracy: 0.9713 - val_loss: 0.7801 - val_accuracy: 0.8624\n",
            "Epoch 147/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.0834 - accuracy: 0.9719 - val_loss: 0.7838 - val_accuracy: 0.8634\n",
            "Epoch 148/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.0836 - accuracy: 0.9717 - val_loss: 0.8392 - val_accuracy: 0.8565\n",
            "Epoch 149/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.0841 - accuracy: 0.9720 - val_loss: 0.8221 - val_accuracy: 0.8568\n",
            "Epoch 150/150\n",
            "553/553 [==============================] - 8s 15ms/step - loss: 0.0847 - accuracy: 0.9715 - val_loss: 0.8127 - val_accuracy: 0.8584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlUzXCWE3wyC"
      },
      "source": [
        "## Evaluating Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfLIFjvewEEB"
      },
      "source": [
        "def decode_sequence(input_seq, encoder_model, decoder_model, reverse_target_char_index):\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index[\"\\t\"]\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        decoder_output = decoder_model.predict([target_seq] + [states_value])\n",
        "        # print(decoder_output)\n",
        "        output_tokens, states = decoder_output[0], decoder_output[1:]\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        # target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [states]\n",
        "    return decoded_sentence    "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 21)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 20, 128)      3456        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 21, 128)      8448        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 20, 128), (N 131584      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 21, 128), (N 131584      embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 21, 66)       8514        lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 283,586\n",
            "Trainable params: 283,586\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2llgOQWpsGO"
      },
      "source": [
        "def evaluate(data_path, encoder_model, decoder_model, batch_size, input_token_index, target_token_index):\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    num_samples = 10000\n",
        "    # Preparing/Loading Test Data\n",
        "    input_texts, target_texts, encoder_input_data, decoder_input_data, decoder_target_data = \\\n",
        "    prepare_test_data(data_path, batch_size, num_samples, input_token_index, target_token_index)\n",
        "    # Reverse-lookup token index to decode sequences back to\n",
        "    # something readable.\n",
        "    reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "    reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "    output_path = '/content/drive/MyDrive/Deep Learning/Output/eng-hi.txt'\n",
        "    correct_predictions = 0\n",
        "    with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
        "      for seq_index in tqdm(range(encoder_input_data.shape[0]), unit='Words', desc='Words Decoded'):\n",
        "        # Take one sequence (part of the training set)\n",
        "        # for trying out decoding.\n",
        "        # print(input_texts[seq_index],target_texts[seq_index])\n",
        "        input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "        decoded_word = decode_sequence(input_seq, encoder_model, decoder_model, reverse_target_char_index).strip(\"\\t\\n \")\n",
        "        f.write(input_texts[seq_index]+'\\t'+decoded_word+'\\n')\n",
        "        # print('<',decoded_word,'><',target_texts[seq_index],'>')\n",
        "        if decoded_word==target_texts[seq_index]:\n",
        "          correct_predictions+=1\n",
        "        # print(\"-\")\n",
        "        # print(\"Input sentence:\", input_texts[seq_index])\n",
        "        # print(\"Decoded sentence:\", decoded_word)\n",
        "      accuracy = correct_predictions/(seq_index+1)\n",
        "      print('Accuracy : ', accuracy,'\\tCorrect Predictions : ',correct_predictions,'/',(seq_index+1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "47127092ab984058954849dafd45ddb5",
            "0fce6c4f12084db7bd7a3f9ba107cb61",
            "99a82bfc9dbe465aa4d9365be76677d5",
            "d7bf75804bf9479db302118f02c4b591",
            "cd7e89b721de44e0873596aac56076e6",
            "502529d68fe4457db32d4f84185e75cb",
            "9fac906cc9b543d99f6453a8bfa4c76d",
            "b33ee04002f64230a0fcd5b033b71f1e"
          ]
        },
        "id": "DmY1j28syi-3",
        "outputId": "324e2b13-d10b-495e-a5da-d28f1470ba26"
      },
      "source": [
        "evaluate(test_data_path, encoder_model, decoder_model, batch_size, input_token_index, target_token_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47127092ab984058954849dafd45ddb5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Words Decoded', max=4502.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 21) for input KerasTensor(type_spec=TensorSpec(shape=(None, 21), dtype=tf.float32, name='input_23'), name='input_23', description=\"created by layer 'input_23'\"), but it was called on an input with incompatible shape (None, 1).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 21) for input KerasTensor(type_spec=TensorSpec(shape=(None, 21), dtype=tf.float32, name='input_23'), name='input_23', description=\"created by layer 'input_23'\"), but it was called on an input with incompatible shape (None, 1).\n",
            "\n",
            "Accuracy :  0.2929808973789427 \tCorrect Predictions :  1319 / 4502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acWeQO1h2L-L"
      },
      "source": [
        "# Question 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hojt1kPVHZf9"
      },
      "source": [
        "## Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS8pFSAgHc0d"
      },
      "source": [
        "class Attention(Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Attention, self).__init__()\n",
        "    self.U = Dense(units)\n",
        "    self.W = Dense(units)\n",
        "    self.V = Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.U(query_with_time_axis) + self.W(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "berPrHVH-kay"
      },
      "source": [
        "## Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGpEjMJk-o-v"
      },
      "source": [
        "def prepare_test_data(data_path, batch_size, num_samples, input_token_index, target_token_index):\n",
        "  with tf.device('/device:GPU:0'):  \n",
        "    input_texts, target_texts, targets = [], [], []\n",
        "    \n",
        "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.read().split(\"\\n\")\n",
        "    for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "        target_text, input_text, _ = line.split(\"\\t\")\n",
        "        # We use \"tab\" as the \"start sequence\" character\n",
        "        # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "        targets.append(target_text.strip(\"\\t\\n \"))\n",
        "        target_text = \"\\t\" + target_text.strip(\"\\t\\n \") + \"\\n\"\n",
        "        input_texts.append(input_text.strip(\"\\t\\n \"))\n",
        "        target_texts.append(target_text)\n",
        "\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_encoder_seq_length), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_target_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "        for t, char in enumerate(input_text):\n",
        "            encoder_input_data[i, t] = input_token_index[char]\n",
        "        encoder_input_data[i, t+1:] = input_token_index[' ']          \n",
        "\n",
        "        for t, char in enumerate(target_text):\n",
        "            decoder_input_data[i, t] = target_token_index[char]\n",
        "        decoder_input_data[i, t+1:] = target_token_index[' ']          \n",
        "\n",
        "        for t, char in enumerate(target_text):        \n",
        "            if t > 0:\n",
        "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "        decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "    return input_texts, targets, encoder_input_data, decoder_input_data, decoder_target_data\n",
        "              "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV_ZLccf2OWd"
      },
      "source": [
        " ## Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mIcdL2S2pVJ"
      },
      "source": [
        "def build_model(encoder_input_size, decoder_input_size, embedding_size, latent_dim, input_vocab, target_vocab, dropout=0.2, cell_type='LSTM', pay_attention=False):\n",
        "  encoder_inputs = Input(shape=(encoder_input_size))\n",
        "  decoder_inputs = Input(shape=(decoder_input_size))\n",
        "  encoder_outputs_i = Input(shape=(latent_dim))\n",
        "  decoder_concat = Concatenate()\n",
        "  \n",
        "  model, encoder_model, decoder_model = None, None, None\n",
        "\n",
        "  if cell_type =='RNN':\n",
        "    # Encoder\n",
        "    outputs, state_h = SimpleRNN(latent_dim, dropout=dropout, return_state=True)(encoder_inputs)\n",
        "    # Decoder\n",
        "    decoder_RNN = SimpleRNN(latent_dim, dropout=dropout, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _ = decoder_RNN(decoder_inputs, initial_state=state_h)\n",
        "    decoder_dense = TimeDistributed(Dense(dencoder_input_size, activation=\"softmax\"))\n",
        "    # Attention\n",
        "    attention = AdditiveAttention(name='attention')    \n",
        "    context_vector, att_weights = attention([decoder_outputs, encoder_outputs], return_attention_scores=True)\n",
        "    decoder_outputs = decoder_concat([decoder_outputs, context_vector])\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    model= Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    # Encoder (Inference)\n",
        "    encoder_model= Model(encoder_inputs, [outputs, state_h])\n",
        "    # Decoder (Inference)\n",
        "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "    decoder_outputs_i, state_h_i = decoder_RNN(decoder_inputs, initial_state=decoder_state_input_h)\n",
        "    # Attention\n",
        "    context_vector_i, att_weights_i = attention([decoder_outputs_i, encoder_outputs_i], return_attention_scores=True)\n",
        "    decoder_outputs_i = decoder_concat([decoder_outputs_i, context_vector_i])\n",
        "    decoder_outputs_i= decoder_dense(decoder_outputs_i)\n",
        "    decoder_model= Model([decoder_inputs, state_h_i, encoder_outputs_i], [decoder_outputs_i, att_weights_i, state_h_i])\n",
        "    \n",
        "  elif cell_type == 'GRU':\n",
        "    # Encoder\n",
        "    encoder_outputs, state_h = GRU(latent_dim, dropout=dropout, return_state=True)(encoder_inputs)\n",
        "    encoder_state = [state_h]\n",
        "    # Decoder\n",
        "    decoder_GRU = GRU(latent_dim, dropout=dropout, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _ = decoder_GRU(decoder_inputs, initial_state=encoder_state)\n",
        "    decoder_dense = TimeDistributed(Dense(dencoder_input_size, activation=\"softmax\"))\n",
        "    # Attention\n",
        "    attention = AdditiveAttention(name='attention')    \n",
        "    context_vector, att_weights = attention([decoder_outputs, encoder_outputs], return_attention_scores=True)\n",
        "    decoder_outputs = decoder_concat([decoder_outputs, context_vector])\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    model= Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    # Encoder (Inference)\n",
        "    encoder_model= keras.models.Model(encoder_inputs, [encoder_outputs, encoder_state])\n",
        "    # Decoder (Inference)\n",
        "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "    decoder_outputs_i, state_h_i = decoder_GRU(decoder_inputs, initial_state=decoder_state_input_h)\n",
        "    # Attention\n",
        "    context_vector_i, att_weights_i = attention([decoder_outputs_i, encoder_outputs_i], return_attention_scores=True)\n",
        "    decoder_outputs_i = decoder_concat([decoder_outputs_i, context_vector_i])\n",
        "    decoder_outputs_i= decoder_dense(decoder_outputs_i)\n",
        "    decoder_model= Model([decoder_inputs, decoder_state_input_h, encoder_outputs_i], [decoder_outputs_i, att_weights_i, state_h_i])\n",
        "\n",
        "  else:\n",
        "    # Encoder\n",
        "    encoder_embeddings = Embedding(input_vocab, embedding_size)(encoder_inputs)\n",
        "    encoder_outputs, state_h, state_c = LSTM(latent_dim, dropout=dropout, return_state=True)(encoder_embeddings)\n",
        "    encoder_states = [state_h, state_c]\n",
        "    # Decoder\n",
        "    decoder_embeddings = Embedding(target_vocab, embedding_size)(decoder_inputs)\n",
        "    decoder_LSTM = LSTM(latent_dim, dropout=dropout, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, state_h_d, state_c_d = decoder_LSTM(decoder_embeddings, initial_state=encoder_states)\n",
        "    # Attention\n",
        "    # attention = AdditiveAttention(name='attention') \n",
        "    attention = Attention(latent_dim)    \n",
        "    repeat_vector = RepeatVector(decoder_input_size)\n",
        "    # context_vector, att_weights = attention([decoder_outputs, encoder_outputs], return_attention_scores=True, training=True)\n",
        "    context_vector, att_weights = attention(encoder_outputs, decoder_outputs)    \n",
        "    context_vector = repeat_vector(context_vector)\n",
        "    decoder_outputs = decoder_concat([decoder_outputs, context_vector])\n",
        "    decoder_dense = TimeDistributed(Dense(target_vocab, activation=\"softmax\"))\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    model= Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    # Encoder (Inference)\n",
        "    encoder_model= Model(encoder_inputs, [encoder_outputs, encoder_states])\n",
        "    # Decoder (Inference)\n",
        "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_outputs_i, state_h_i, state_c_i = decoder_LSTM(decoder_embeddings, initial_state=decoder_states_inputs)\n",
        "    decoder_states_i = [state_h_i, state_c_i]\n",
        "    # Attention\n",
        "    print('Inference Decoder: ',encoder_outputs_i.shape, decoder_outputs_i.shape)\n",
        "    context_vector_i, att_weights_i = attention(encoder_outputs_i, decoder_outputs_i)\n",
        "    context_vector_i = repeat_vector(context_vector_i)\n",
        "    print(decoder_outputs_i.shape, context_vector_i.shape, att_weights_i.shape)\n",
        "    decoder_outputs_i = decoder_concat([decoder_outputs_i, context_vector_i])    \n",
        "    decoder_outputs_i = decoder_dense(decoder_outputs_i)\n",
        "    decoder_model = Model([decoder_inputs] + decoder_states_inputs + [encoder_outputs_i], [decoder_outputs_i] + [att_weights_i] + decoder_states_i)\n",
        "\n",
        "  return model, encoder_model, decoder_model"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKKp0EIO2TZr"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ivqzg3xa2p5R",
        "outputId": "2fd36c56-bd23-48ed-8daf-c9aae3a01923"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  # Parameters\n",
        "  batch_size = 64\n",
        "  epochs = 1\n",
        "  latent_dim = 64  # hidden layer size\n",
        "  embedding_size = 256\n",
        "  hidden_layers = 2\n",
        "  cell_type = 'LSTM'\n",
        "  dropout = 0.3\n",
        "  rec_dropout = 0\n",
        "  num_samples = 44205\n",
        "\n",
        "  # Path to the data txt file on disk.\n",
        "  train_data_path = \"/content/drive/MyDrive/Deep Learning/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "  test_data_path = \"/content/drive/MyDrive/Deep Learning/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\"\n",
        "\n",
        "  # Preparing/Loading Data\n",
        "  input_texts, target_texts, encoder_input_data, decoder_input_data, decoder_target_data, num_encoder_tokens, num_decoder_tokens,\\\n",
        "        max_encoder_seq_length, max_decoder_seq_length, input_token_index, target_token_index = prepare_data(train_data_path, batch_size, num_samples)\n",
        "\n",
        "  # Building Model\n",
        "  model, encoder_model, decoder_model = build_model(max_encoder_seq_length, max_decoder_seq_length, embedding_size,\\\n",
        "                                                      latent_dim, num_encoder_tokens, num_decoder_tokens, dropout=0.3, cell_type='LSTM', pay_attention=True)\n",
        "\n",
        "  # Training Model\n",
        "  model.compile(\n",
        "      optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "  )\n",
        "  history = model.fit(\n",
        "      [encoder_input_data, decoder_input_data],\n",
        "      decoder_target_data,\n",
        "      batch_size=batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_split=0.2,\n",
        "      # callbacks=[EarlyStopping(monitor='val_loss', verbose=1)]\n",
        "  )"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Inference Decoder:  (None, 64) (None, 21, 64)\n",
            "(None, 21, 64) (None, 21, 64) (None, 21, 1)\n",
            "553/553 [==============================] - 59s 97ms/step - loss: 1.2018 - accuracy: 0.7226 - val_loss: 0.6561 - val_accuracy: 0.8327\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ICz2kpueC0V"
      },
      "source": [
        "target_texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcqFciXp2cTu"
      },
      "source": [
        "## Predicting/Decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8FU5FEZ2qgl"
      },
      "source": [
        "def decode_sequence(input_seq, encoder_model, decoder_model, reverse_target_char_index):\n",
        "    # Encode the input as state vectors.\n",
        "    encoder_outputs = encoder_model.predict(input_seq)\n",
        "    encoder_output = encoder_outputs[0]\n",
        "    encoder_states = encoder_outputs[1:]\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 21))\n",
        "    target_seq[0, 0] = target_token_index[\"\\t\"]\n",
        "    attention_weights = []\n",
        "    states_value = encoder_states\n",
        "    stop_condition = False\n",
        "    decoded_word = \"\"\n",
        "    while not stop_condition:\n",
        "        decoder_outputs = decoder_model.predict([target_seq, states_value, encoder_output])\n",
        "        output_tokens = decoder_outputs[0]\n",
        "        states_value = decoder_outputs[2:]\n",
        "        print(output_tokens)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_word += sampled_char\n",
        "        print('Predicted: <',sampled_token_index,sampled_char,'>')\n",
        "        attention_weights.append((sampled_token_index, decoder_outputs[1]))\n",
        "        # print(decoder_outputs[1])\n",
        "        if sampled_char == \"\\n\" or len(decoded_word) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = np.zeros((1, 21))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        \n",
        "    return decoded_word, attention_weights    "
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5tfoUeZcQ-g"
      },
      "source": [
        "## Evaluating Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZogIZYlJcTYO"
      },
      "source": [
        "def evaluate(data_path, encoder_model, decoder_model, batch_size, input_token_index, target_token_index):\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    num_samples = 10000\n",
        "    # Preparing/Loading Test Data\n",
        "    input_texts, target_texts, encoder_input_data, decoder_input_data, decoder_target_data = \\\n",
        "    prepare_test_data(data_path, batch_size, num_samples, input_token_index, target_token_index)\n",
        "    # Reverse-lookup token index to decode sequences back to\n",
        "    # something readable.\n",
        "    reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "    reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "    output_path = '/content/drive/MyDrive/Deep Learning/Output/eng-hi-att.txt'\n",
        "    correct_predictions = 0\n",
        "    with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
        "      for seq_index in tqdm(range(10), unit='Words', desc='Words Decoded'): #encoder_input_data.shape[0]\n",
        "        # Take one sequence (part of the training set)\n",
        "        # for trying out decoding.\n",
        "        # print(input_texts[seq_index],target_texts[seq_index])\n",
        "        input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "        decoded_word, _ = decode_sequence(input_seq, encoder_model, decoder_model, reverse_target_char_index)\n",
        "        decoded_word = decoded_word.strip(\"\\t\\n \")\n",
        "        f.write(input_texts[seq_index]+'\\t'+decoded_word+'\\n')\n",
        "        # print('<',decoded_word,'><',target_texts[seq_index],'>')\n",
        "        if decoded_word==target_texts[seq_index]:\n",
        "          correct_predictions+=1\n",
        "        # print(\"-\")\n",
        "        # print(\"Input sentence:\", input_texts[seq_index])\n",
        "        # print(\"Decoded sentence:\", decoded_word)\n",
        "      accuracy = correct_predictions/(seq_index+1)\n",
        "      print('Accuracy : ', accuracy,'\\tCorrect Predictions : ',correct_predictions,'/',(seq_index+1))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnXhzgp6KYc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "da683327fe1d42a2a907302bcf2e6394",
            "010bc0799d614441b874a14e345d9b83",
            "8c630759cdcd432fb3cecf2c4d868277",
            "4e561f61cd114375a2e4c16639bd6e31",
            "6af4babe8d6d409aa59e73b575242a85",
            "66f94f9bcf134d93bdc7da566352a8ba",
            "0bb2fd9c738147faaf14a33dd66804a7",
            "b6ec847a5ee9466d8570407ab7b7e44d"
          ]
        },
        "outputId": "caa73a01-0354-45ce-8d73-926c0b99ebc5"
      },
      "source": [
        "evaluate(test_data_path, encoder_model, decoder_model, batch_size, input_token_index, target_token_index)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da683327fe1d42a2a907302bcf2e6394",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Words Decoded', max=4502.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy :  0.0 \tCorrect Predictions :  0 / 4502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGK1SCX_2oP_"
      },
      "source": [
        "## Attention Heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XogzsX_q_WuE"
      },
      "source": [
        "def plot_heatmaps(data_path, encoder_model, decoder_model, batch_size, input_token_index, target_token_index):\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    num_samples = 10000\n",
        "    # Preparing/Loading Test Data\n",
        "    input_texts, target_texts, encoder_input_data, decoder_input_data, decoder_target_data = \\\n",
        "    prepare_test_data(data_path, batch_size, num_samples, input_token_index, target_token_index)\n",
        "    # Reverse-lookup token index to decode sequences back to\n",
        "    # something readable.\n",
        "    reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "    reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "    correct_predictions = 0\n",
        "    plt.figure(figsize=(40, 16))\n",
        "    font = FontProperties(fname='/content/Lohit-Devanagari.ttf', size=36)\n",
        "    for seq_index in tqdm(range(1), unit='Words', desc='Words Decoded'):\n",
        "      input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "      decoded_word, att_weights = decode_sequence(input_seq, encoder_model, decoder_model, reverse_target_char_index)\n",
        "      decoded_word = decoded_word.strip(\"\\t\\n \")\n",
        "      \n",
        "      mats = []\n",
        "      dec_inputs = []\n",
        "      for dec_ind, attn in att_weights:\n",
        "        mats.append(attn.reshape(-1))\n",
        "        dec_inputs.append(dec_ind)\n",
        "      att_mat = np.transpose(np.array(mats))\n",
        "      xlabels = [char for char in input_texts[seq_index]]\n",
        "      ylabels = [char for char in decoded_word]\n",
        "      print(att_mat.shape)\n",
        "      # att_matrix = np.random.rand(len(ylabels), len(xlabels))\n",
        "      # for i in range(len(ylabels)):\n",
        "      #   for j in range(len(xlabels)):\n",
        "      #     att_matrix[i][j] = \n",
        "      # print(xlabels, ylabels, att_mat)\n",
        "      # Heatmap\n",
        "      plt.subplot(2, 5, seq_index+1)\n",
        "      plt.imshow(att_mat)\n",
        "      plt.xticks(ticks=range(len(xlabels)), labels=xlabels,fontproperties=FontProperties(size=36))\n",
        "      plt.yticks(ticks=range(len(ylabels)),labels=ylabels, fontproperties=font)\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dQHmXX38irh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6962e09ae59d452a869d83ab8a82becd",
            "2e59bc5b897f47fa90718f50fc7670ab",
            "8ef120bf435d41adada129422e713a69",
            "f5af25ddaf9e479295ee5b2b9b4fc984",
            "ba6acd0c16d44c5284d28cd34a5139c8",
            "b5bb52fbaccd42e6a81db3cc6f17c357",
            "87a49e4697ad4d628bc837339de14d36",
            "8d34a96bd1a1437b87c4ea91580c88dc"
          ]
        },
        "outputId": "aea66c8c-12a3-4cef-af7b-48ad38809aad"
      },
      "source": [
        "plot_heatmaps(test_data_path, encoder_model, decoder_model, batch_size, input_token_index, target_token_index)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6962e09ae59d452a869d83ab8a82becd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Words Decoded', max=1.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[[[9.9543158e-06 3.6325837e-03 4.0140230e-07 ... 1.2663794e-03\n",
            "   3.3648036e-04 1.7610801e-05]\n",
            "  [1.6306971e-05 1.6307771e-03 1.0346024e-02 ... 1.5700479e-03\n",
            "   1.6076193e-03 2.8861768e-05]\n",
            "  [1.6324850e-06 1.0244768e-04 9.7562271e-01 ... 3.5028617e-05\n",
            "   9.7412201e-05 2.4570741e-06]\n",
            "  ...\n",
            "  [2.3115478e-11 3.0688334e-08 9.9999988e-01 ... 2.2384769e-11\n",
            "   1.6027712e-09 3.3276135e-11]\n",
            "  [2.2489233e-11 2.9812480e-08 9.9999988e-01 ... 2.1324893e-11\n",
            "   1.5566238e-09 3.2206168e-11]\n",
            "  [2.1985079e-11 2.9081189e-08 9.9999988e-01 ... 2.0463445e-11\n",
            "   1.5181187e-09 3.1335368e-11]]]\n",
            "Predicted: < 2   >\n",
            "[[[6.5059985e-10 6.1789813e-04 9.9937016e-01 ... 3.7488046e-08\n",
            "   3.4436088e-08 2.2579580e-10]\n",
            "  [4.7778592e-10 6.6352932e-04 9.9932349e-01 ... 3.4895514e-08\n",
            "   4.3361457e-08 1.7612234e-10]\n",
            "  [4.2459353e-10 5.9854798e-04 9.9939024e-01 ... 3.0228712e-08\n",
            "   3.7455639e-08 1.5522833e-10]\n",
            "  ...\n",
            "  [4.7356935e-10 7.3321111e-04 9.9925572e-01 ... 3.1888231e-08\n",
            "   3.5281030e-08 1.7495433e-10]\n",
            "  [4.7728510e-10 7.4144325e-04 9.9924743e-01 ... 3.2029259e-08\n",
            "   3.5215592e-08 1.7634927e-10]\n",
            "  [4.8072496e-10 7.4900943e-04 9.9923992e-01 ... 3.2150634e-08\n",
            "   3.5145138e-08 1.7762598e-10]]]\n",
            "Predicted: < 2   >\n",
            "[[[1.5321912e-09 1.2274381e-03 9.9875426e-01 ... 7.1201953e-08\n",
            "   6.4585130e-08 5.3747334e-10]\n",
            "  [1.0395625e-09 1.2439534e-03 9.9873763e-01 ... 5.9500795e-08\n",
            "   7.6057255e-08 3.8836279e-10]\n",
            "  [8.9536006e-10 1.0552997e-03 9.9892896e-01 ... 4.9107609e-08\n",
            "   6.3525363e-08 3.2845418e-10]\n",
            "  ...\n",
            "  [6.2266259e-10 6.4817013e-04 9.9933964e-01 ... 3.3664371e-08\n",
            "   5.2498745e-08 2.1986798e-10]\n",
            "  [6.1344368e-10 6.3766621e-04 9.9935025e-01 ... 3.3267828e-08\n",
            "   5.2231925e-08 2.1649416e-10]\n",
            "  [6.0502164e-10 6.2813808e-04 9.9936002e-01 ... 3.2905536e-08\n",
            "   5.1984959e-08 2.1341642e-10]]]\n",
            "Predicted: < 2   >\n",
            "[[[1.9147655e-09 1.3389789e-03 9.9864155e-01 ... 7.7589391e-08\n",
            "   7.0698654e-08 6.6809924e-10]\n",
            "  [1.2160398e-09 1.2931691e-03 9.9868828e-01 ... 6.0722748e-08\n",
            "   7.9198756e-08 4.5288234e-10]\n",
            "  [1.0213783e-09 1.0680705e-03 9.9891651e-01 ... 4.9049707e-08\n",
            "   6.5154794e-08 3.7328107e-10]\n",
            "  ...\n",
            "  [6.1217376e-10 5.6477985e-04 9.9942362e-01 ... 3.1473711e-08\n",
            "   5.3350313e-08 2.1717098e-10]\n",
            "  [6.0263561e-10 5.5548566e-04 9.9943286e-01 ... 3.1132959e-08\n",
            "   5.3142912e-08 2.1373997e-10]\n",
            "  [5.9414806e-10 5.4726587e-04 9.9944133e-01 ... 3.0829021e-08\n",
            "   5.2955663e-08 2.1068906e-10]]]\n",
            "Predicted: < 2   >\n",
            "[[[1.8429728e-09 1.2879474e-03 9.9869341e-01 ... 7.4370362e-08\n",
            "   6.7794851e-08 6.3945960e-10]\n",
            "  [1.1708708e-09 1.2427525e-03 9.9873930e-01 ... 5.8225972e-08\n",
            "   7.5990826e-08 4.3383039e-10]\n",
            "  [9.8225161e-10 1.0249976e-03 9.9896026e-01 ... 4.7035606e-08\n",
            "   6.2555344e-08 3.5723985e-10]\n",
            "  ...\n",
            "  [5.9538768e-10 5.4981641e-04 9.9943894e-01 ... 3.0722177e-08\n",
            "   5.1846271e-08 2.1068655e-10]\n",
            "  [5.8686561e-10 5.4156693e-04 9.9944729e-01 ... 3.0428222e-08\n",
            "   5.1682310e-08 2.0763961e-10]\n",
            "  [5.7931399e-10 5.3430529e-04 9.9945456e-01 ... 3.0167584e-08\n",
            "   5.1535427e-08 2.0494234e-10]]]\n",
            "Predicted: < 2   >\n",
            "[[[1.7886602e-09 1.2597304e-03 9.9872190e-01 ... 7.2573059e-08\n",
            "   6.6137787e-08 6.1902816e-10]\n",
            "  [1.1407172e-09 1.2181770e-03 9.9876416e-01 ... 5.7020120e-08\n",
            "   7.4352940e-08 4.2160958e-10]\n",
            "  [9.5817221e-10 1.0058010e-03 9.9897951e-01 ... 4.6122608e-08\n",
            "   6.1288354e-08 3.4765635e-10]\n",
            "  ...\n",
            "  [5.8817379e-10 5.4651359e-04 9.9944228e-01 ... 3.0463490e-08\n",
            "   5.1160477e-08 2.0777524e-10]\n",
            "  [5.8008542e-10 5.3864327e-04 9.9945015e-01 ... 3.0187309e-08\n",
            "   5.1013060e-08 2.0489296e-10]\n",
            "  [5.7293542e-10 5.3173228e-04 9.9945706e-01 ... 2.9943283e-08\n",
            "   5.0882033e-08 2.0234706e-10]]]\n",
            "Predicted: < 2   >\n",
            "[[[1.7670587e-09 1.2489255e-03 9.9873275e-01 ... 7.1875284e-08\n",
            "   6.5494511e-08 6.1095534e-10]\n",
            "  [1.1287742e-09 1.2089177e-03 9.9877352e-01 ... 5.6559003e-08\n",
            "   7.3725630e-08 4.1679302e-10]\n",
            "  [9.4870101e-10 9.9866325e-04 9.9898678e-01 ... 4.5776325e-08\n",
            "   6.0806506e-08 3.4390160e-10]\n",
            "  ...\n",
            "  [5.8545685e-10 5.4549275e-04 9.9944335e-01 ... 3.0370060e-08\n",
            "   5.0906184e-08 2.0667942e-10]\n",
            "  [5.7753574e-10 5.3776248e-04 9.9945110e-01 ... 3.0100523e-08\n",
            "   5.0765113e-08 2.0386015e-10]\n",
            "  [5.7053368e-10 5.3097645e-04 9.9945801e-01 ... 2.9862495e-08\n",
            "   5.0639741e-08 2.0137085e-10]]]\n",
            "Predicted: < 2   >\n",
            "[[[1.7591197e-09 1.2450193e-03 9.9873680e-01 ... 7.1620356e-08\n",
            "   6.5259975e-08 6.0799227e-10]\n",
            "  [1.1243775e-09 1.2055830e-03 9.9877697e-01 ... 5.6391265e-08\n",
            "   7.3497880e-08 4.1502227e-10]\n",
            "  [9.4522645e-10 9.9611469e-04 9.9898928e-01 ... 4.5651142e-08\n",
            "   6.0632594e-08 3.4252445e-10]\n",
            "  ...\n",
            "  [5.8447969e-10 5.4516480e-04 9.9944371e-01 ... 3.0337244e-08\n",
            "   5.0815501e-08 2.0628527e-10]\n",
            "  [5.7661353e-10 5.3747935e-04 9.9945134e-01 ... 3.0069831e-08\n",
            "   5.0676320e-08 2.0348727e-10]\n",
            "  [5.6966709e-10 5.3073914e-04 9.9945825e-01 ... 2.9834037e-08\n",
            "   5.0553094e-08 2.0101901e-10]]]\n",
            "Predicted: < 2   >\n",
            "[[[1.7562961e-09 1.2436465e-03 9.9873835e-01 ... 7.1530089e-08\n",
            "   6.5177105e-08 6.0694116e-10]\n",
            "  [1.1228228e-09 1.2044287e-03 9.9877805e-01 ... 5.6332848e-08\n",
            "   7.3418647e-08 4.1439863e-10]\n",
            "  [9.4398611e-10 9.9522329e-04 9.9899036e-01 ... 4.5607067e-08\n",
            "   6.0571388e-08 3.4203648e-10]\n",
            "  ...\n",
            "  [5.8413641e-10 5.4505980e-04 9.9944371e-01 ... 3.0325964e-08\n",
            "   5.0783918e-08 2.0614800e-10]\n",
            "  [5.7629362e-10 5.3739408e-04 9.9945146e-01 ... 3.0059450e-08\n",
            "   5.0645692e-08 2.0335887e-10]\n",
            "  [5.6936411e-10 5.3066807e-04 9.9945837e-01 ... 2.9824371e-08\n",
            "   5.0523120e-08 2.0089713e-10]]]\n",
            "Predicted: < 2   >\n",
            "[[[1.7553158e-09 1.2431769e-03 9.9873883e-01 ... 7.1499024e-08\n",
            "   6.5148548e-08 6.0657462e-10]\n",
            "  [1.1222794e-09 1.2040314e-03 9.9877852e-01 ... 5.6312462e-08\n",
            "   7.3391242e-08 4.1417991e-10]\n",
            "  [9.4356156e-10 9.9492457e-04 9.9899060e-01 ... 4.5592028e-08\n",
            "   6.0550725e-08 3.4186701e-10]\n",
            "  ...\n",
            "  [5.8402061e-10 5.4502866e-04 9.9944383e-01 ... 3.0322205e-08\n",
            "   5.0773362e-08 2.0610125e-10]\n",
            "  [5.7618477e-10 5.3736795e-04 9.9945146e-01 ... 3.0056015e-08\n",
            "   5.0635357e-08 2.0331466e-10]\n",
            "  [5.6926097e-10 5.3064636e-04 9.9945837e-01 ... 2.9821127e-08\n",
            "   5.0512909e-08 2.0085539e-10]]]\n",
            "Predicted: < 2   >\n",
            "[[[1.7549913e-09 1.2430211e-03 9.9873894e-01 ... 7.1488671e-08\n",
            "   6.5139119e-08 6.0645439e-10]\n",
            "  [1.1221020e-09 1.2039013e-03 9.9877864e-01 ... 5.6305808e-08\n",
            "   7.3382154e-08 4.1410730e-10]\n",
            "  [9.4341401e-10 9.9481840e-04 9.9899071e-01 ... 4.5586816e-08\n",
            "   6.0543464e-08 3.4180836e-10]\n",
            "  ...\n",
            "  [5.8398053e-10 5.4501748e-04 9.9944383e-01 ... 3.0320937e-08\n",
            "   5.0769685e-08 2.0608512e-10]\n",
            "  [5.7614746e-10 5.3735822e-04 9.9945146e-01 ... 3.0054810e-08\n",
            "   5.0631687e-08 2.0329914e-10]\n",
            "  [5.6922622e-10 5.3063873e-04 9.9945837e-01 ... 2.9820043e-08\n",
            "   5.0509438e-08 2.0084120e-10]]]\n",
            "Predicted: < 2   >\n",
            "[[[1.7548674e-09 1.2429584e-03 9.9873894e-01 ... 7.1484578e-08\n",
            "   6.5135509e-08 6.0640931e-10]\n",
            "  [1.1220356e-09 1.2038468e-03 9.9877864e-01 ... 5.6303232e-08\n",
            "   7.3378793e-08 4.1408044e-10]\n",
            "  [9.4336550e-10 9.9478150e-04 9.9899083e-01 ... 4.5585082e-08\n",
            "   6.0541048e-08 3.4178885e-10]\n",
            "  ...\n",
            "  [5.8396493e-10 5.4501131e-04 9.9944383e-01 ... 3.0320358e-08\n",
            "   5.0768328e-08 2.0607924e-10]\n",
            "  [5.7613314e-10 5.3735333e-04 9.9945146e-01 ... 3.0054348e-08\n",
            "   5.0630433e-08 2.0329410e-10]\n",
            "  [5.6921429e-10 5.3063524e-04 9.9945837e-01 ... 2.9819589e-08\n",
            "   5.0508188e-08 2.0083622e-10]]]\n",
            "Predicted: < 2   >\n",
            "[[[1.7548375e-09 1.2429454e-03 9.9873906e-01 ... 7.1483768e-08\n",
            "   6.5134650e-08 6.0639660e-10]\n",
            "  [1.1220164e-09 1.2038335e-03 9.9877864e-01 ... 5.6302593e-08\n",
            "   7.3377819e-08 4.1407255e-10]\n",
            "  [9.4334940e-10 9.9477160e-04 9.9899083e-01 ... 4.5584475e-08\n",
            "   6.0540238e-08 3.4178302e-10]\n",
            "  ...\n",
            "  [5.8396271e-10 5.4501154e-04 9.9944383e-01 ... 3.0320358e-08\n",
            "   5.0768037e-08 2.0607767e-10]\n",
            "  [5.7612987e-10 5.3735333e-04 9.9945146e-01 ... 3.0054235e-08\n",
            "   5.0630042e-08 2.0329255e-10]\n",
            "  [5.6920990e-10 5.3063495e-04 9.9945837e-01 ... 2.9819532e-08\n",
            "   5.0507804e-08 2.0083470e-10]]]\n",
            "Predicted: < 2   >\n",
            "[[[1.7548307e-09 1.2429424e-03 9.9873906e-01 ... 7.1483491e-08\n",
            "   6.5134408e-08 6.0639427e-10]\n",
            "  [1.1220143e-09 1.2038352e-03 9.9877864e-01 ... 5.6302593e-08\n",
            "   7.3377819e-08 4.1407255e-10]\n",
            "  [9.4334585e-10 9.9476916e-04 9.9899083e-01 ... 4.5584386e-08\n",
            "   6.0540124e-08 3.4178169e-10]\n",
            "  ...\n",
            "  [5.8396160e-10 5.4501207e-04 9.9944383e-01 ... 3.0320301e-08\n",
            "   5.0767941e-08 2.0607767e-10]\n",
            "  [5.7612876e-10 5.3735304e-04 9.9945146e-01 ... 3.0054178e-08\n",
            "   5.0629950e-08 2.0329216e-10]\n",
            "  [5.6920990e-10 5.3063571e-04 9.9945837e-01 ... 2.9819532e-08\n",
            "   5.0507804e-08 2.0083470e-10]]]\n",
            "Predicted: < 2   >\n",
            "[[[1.7548241e-09 1.2429395e-03 9.9873906e-01 ... 7.1483356e-08\n",
            "   6.5134280e-08 6.0639194e-10]\n",
            "  [1.1220143e-09 1.2038347e-03 9.9877864e-01 ... 5.6302593e-08\n",
            "   7.3377819e-08 4.1407178e-10]\n",
            "  [9.4334585e-10 9.9476823e-04 9.9899083e-01 ... 4.5584301e-08\n",
            "   6.0540124e-08 3.4178105e-10]\n",
            "  ...\n",
            "  [5.8396044e-10 5.4501207e-04 9.9944383e-01 ... 3.0320358e-08\n",
            "   5.0767941e-08 2.0607728e-10]\n",
            "  [5.7612876e-10 5.3735333e-04 9.9945146e-01 ... 3.0054235e-08\n",
            "   5.0629950e-08 2.0329177e-10]\n",
            "  [5.6920885e-10 5.3063600e-04 9.9945837e-01 ... 2.9819532e-08\n",
            "   5.0507804e-08 2.0083431e-10]]]\n",
            "Predicted: < 2   >\n",
            "[[[1.7548241e-09 1.2429401e-03 9.9873906e-01 ... 7.1483356e-08\n",
            "   6.5134280e-08 6.0639194e-10]\n",
            "  [1.1220143e-09 1.2038358e-03 9.9877864e-01 ... 5.6302593e-08\n",
            "   7.3377819e-08 4.1407178e-10]\n",
            "  [9.4334585e-10 9.9476869e-04 9.9899083e-01 ... 4.5584386e-08\n",
            "   6.0540124e-08 3.4178105e-10]\n",
            "  ...\n",
            "  [5.8396160e-10 5.4501288e-04 9.9944383e-01 ... 3.0320358e-08\n",
            "   5.0767941e-08 2.0607767e-10]\n",
            "  [5.7612876e-10 5.3735386e-04 9.9945146e-01 ... 3.0054235e-08\n",
            "   5.0629950e-08 2.0329216e-10]\n",
            "  [5.6920990e-10 5.3063675e-04 9.9945837e-01 ... 2.9819532e-08\n",
            "   5.0507804e-08 2.0083507e-10]]]\n",
            "Predicted: < 2   >\n",
            "[[[1.7548207e-09 1.2429382e-03 9.9873906e-01 ... 7.1483356e-08\n",
            "   6.5134280e-08 6.0639194e-10]\n",
            "  [1.1220120e-09 1.2038335e-03 9.9877864e-01 ... 5.6302483e-08\n",
            "   7.3377819e-08 4.1407178e-10]\n",
            "  [9.4334407e-10 9.9476730e-04 9.9899083e-01 ... 4.5584301e-08\n",
            "   6.0540003e-08 3.4178105e-10]\n",
            "  ...\n",
            "  [5.8396044e-10 5.4501079e-04 9.9944383e-01 ... 3.0320244e-08\n",
            "   5.0767845e-08 2.0607688e-10]\n",
            "  [5.7612765e-10 5.3735258e-04 9.9945146e-01 ... 3.0054178e-08\n",
            "   5.0629851e-08 2.0329177e-10]\n",
            "  [5.6920885e-10 5.3063524e-04 9.9945837e-01 ... 2.9819475e-08\n",
            "   5.0507708e-08 2.0083431e-10]]]\n",
            "Predicted: < 2   >\n",
            "[[[1.7548207e-09 1.2429382e-03 9.9873906e-01 ... 7.1483221e-08\n",
            "   6.5134280e-08 6.0639194e-10]\n",
            "  [1.1220120e-09 1.2038335e-03 9.9877864e-01 ... 5.6302483e-08\n",
            "   7.3377819e-08 4.1407178e-10]\n",
            "  [9.4334218e-10 9.9476497e-04 9.9899083e-01 ... 4.5584216e-08\n",
            "   6.0539890e-08 3.4177974e-10]\n",
            "  ...\n",
            "  [5.8396044e-10 5.4501131e-04 9.9944383e-01 ... 3.0320301e-08\n",
            "   5.0767845e-08 2.0607728e-10]\n",
            "  [5.7612876e-10 5.3735333e-04 9.9945146e-01 ... 3.0054235e-08\n",
            "   5.0629950e-08 2.0329177e-10]\n",
            "  [5.6920990e-10 5.3063571e-04 9.9945837e-01 ... 2.9819532e-08\n",
            "   5.0507804e-08 2.0083431e-10]]]\n",
            "Predicted: < 2   >\n",
            "[[[1.7548241e-09 1.2429407e-03 9.9873906e-01 ... 7.1483356e-08\n",
            "   6.5134280e-08 6.0639194e-10]\n",
            "  [1.1220120e-09 1.2038330e-03 9.9877864e-01 ... 5.6302483e-08\n",
            "   7.3377677e-08 4.1407097e-10]\n",
            "  [9.4334585e-10 9.9476869e-04 9.9899083e-01 ... 4.5584386e-08\n",
            "   6.0540003e-08 3.4178105e-10]\n",
            "  ...\n",
            "  [5.8396044e-10 5.4501154e-04 9.9944383e-01 ... 3.0320301e-08\n",
            "   5.0767845e-08 2.0607688e-10]\n",
            "  [5.7612765e-10 5.3735357e-04 9.9945146e-01 ... 3.0054178e-08\n",
            "   5.0629950e-08 2.0329177e-10]\n",
            "  [5.6920885e-10 5.3063571e-04 9.9945837e-01 ... 2.9819475e-08\n",
            "   5.0507708e-08 2.0083431e-10]]]\n",
            "Predicted: < 2   >\n",
            "[[[1.7548207e-09 1.2429382e-03 9.9873906e-01 ... 7.1483221e-08\n",
            "   6.5134152e-08 6.0639194e-10]\n",
            "  [1.1220120e-09 1.2038330e-03 9.9877864e-01 ... 5.6302483e-08\n",
            "   7.3377677e-08 4.1407097e-10]\n",
            "  [9.4334407e-10 9.9476636e-04 9.9899083e-01 ... 4.5584301e-08\n",
            "   6.0540003e-08 3.4178041e-10]\n",
            "  ...\n",
            "  [5.8396044e-10 5.4501079e-04 9.9944383e-01 ... 3.0320244e-08\n",
            "   5.0767845e-08 2.0607688e-10]\n",
            "  [5.7612765e-10 5.3735258e-04 9.9945146e-01 ... 3.0054178e-08\n",
            "   5.0629851e-08 2.0329177e-10]\n",
            "  [5.6920885e-10 5.3063472e-04 9.9945837e-01 ... 2.9819475e-08\n",
            "   5.0507708e-08 2.0083431e-10]]]\n",
            "Predicted: < 2   >\n",
            "[[[1.7548241e-09 1.2429401e-03 9.9873906e-01 ... 7.1483356e-08\n",
            "   6.5134280e-08 6.0639194e-10]\n",
            "  [1.1220143e-09 1.2038358e-03 9.9877864e-01 ... 5.6302593e-08\n",
            "   7.3377819e-08 4.1407178e-10]\n",
            "  [9.4334218e-10 9.9476683e-04 9.9899083e-01 ... 4.5584301e-08\n",
            "   6.0539890e-08 3.4177974e-10]\n",
            "  ...\n",
            "  [5.8396044e-10 5.4501183e-04 9.9944383e-01 ... 3.0320301e-08\n",
            "   5.0767845e-08 2.0607688e-10]\n",
            "  [5.7612765e-10 5.3735386e-04 9.9945146e-01 ... 3.0054178e-08\n",
            "   5.0629950e-08 2.0329177e-10]\n",
            "  [5.6920885e-10 5.3063623e-04 9.9945837e-01 ... 2.9819532e-08\n",
            "   5.0507708e-08 2.0083431e-10]]]\n",
            "Predicted: < 2   >\n",
            "[[[1.7548274e-09 1.2429418e-03 9.9873906e-01 ... 7.1483491e-08\n",
            "   6.5134408e-08 6.0639316e-10]\n",
            "  [1.1220120e-09 1.2038330e-03 9.9877864e-01 ... 5.6302483e-08\n",
            "   7.3377677e-08 4.1407097e-10]\n",
            "  [9.4334585e-10 9.9476823e-04 9.9899083e-01 ... 4.5584386e-08\n",
            "   6.0540124e-08 3.4178105e-10]\n",
            "  ...\n",
            "  [5.8396160e-10 5.4501183e-04 9.9944383e-01 ... 3.0320301e-08\n",
            "   5.0767941e-08 2.0607767e-10]\n",
            "  [5.7612876e-10 5.3735304e-04 9.9945146e-01 ... 3.0054178e-08\n",
            "   5.0629950e-08 2.0329177e-10]\n",
            "  [5.6920990e-10 5.3063571e-04 9.9945837e-01 ... 2.9819532e-08\n",
            "   5.0507708e-08 2.0083431e-10]]]\n",
            "Predicted: < 2   >\n",
            "(21, 22)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2880x1152 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}